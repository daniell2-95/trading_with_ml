from xgboost import XGBClassifier, XGBRegressor
from sklearn.svm import SVC
from typing import Any, Dict, Union, List
from sklearn.metrics import roc_auc_score, accuracy_score
from collections import defaultdict

from model_definition.ma_strategy import MovingAverageStrategy
 
import pandas as pd
import numpy as np
import mlflow
import hyperopt
import random
import string
import os
import yaml

from copy import deepcopy
import warnings
warnings.filterwarnings("ignore")
 
# Model attributes for different model flavors. Check out https://www.mlflow.org/docs/latest/models.html for documentation
# on which models can be supported by MLflow

MODEL_ATTRIBUTES = {
    'direction': {
        'ML': {
            'xgboost': {
                'model_object': XGBClassifier(random_state = 42),
                'mlflow_class': 'mlflow.xgboost'
            },
            'svm': {
                'model_object': SVC(max_iter = 5000, probability = True, random_state = 42),
                'mlflow_class': 'mlflow.sklearn'
            }
        },
        'algo': {
            'moving_average': {
                'model_object': MovingAverageStrategy(),
                'mlflow_class': 'mlflow.pyfunc'
            }
        }
    },
    'volatility_t_5': {
        'xgboost': {
            'model_object': XGBRegressor(random_state = 42),
            'mlflow_class': 'mlflow.xgboost'
        }
    }
}

 

SKLEARN_METRICS_DICT =  {"roc_auc": roc_auc_score,
                         "accuracy": accuracy_score}

# Load config
with open(os.path.dirname(os.path.dirname(__file__)) + '/config.yaml', 'r') as file:
	config = yaml.safe_load(file)

MLFLOW_TRACKING_URI = config["MLFLOW_TRACKING_URI"]

 
def build_train_objective(train_data: np.array,
                          scores: List[str],
                          features: List[str],
                          target: str,
                          model_type: str,
                          model_class: str,
                          horizon: int,
                          window: int,
                          run_name: str):
    
    """
    Returns a function that trains a model and returns a loss metric.

    Parameters:
    -----------
    train_data: np.array
        A numpy array of shape (n_samples, n_features) containing the training data.
    scores: List[str]
        A list of strings containing the score names to use for evaluation of the model.
    features: List[str]
        A list of strings containing the names of the features to use for training the model.
    target: str
        A string containing the name of the target variable to predict.
    model_type: str
        A string indicating the type of model to use for training (e.g. 'algo', 'ML').
    model_class: str
        A string indicating the specific class of model to use for training (e.g. 'RandomForest', 'LogisticRegression').
    horizon: int
        An integer indicating the number of time steps ahead to predict.
    window: int
        An integer indicating the length of the rolling window used to generate the training data.
    run_name: str
        A string indicating the name of the run to be used for logging.

    Returns:
    --------
    Callable[[Dict[str, Any]], Dict[str, Any]] : A function that takes a dictionary of hyperparameters as 
                                                 input and returns a dictionary containing the status of 
                                                 the run and the loss function.

    """
    
    def _train_func(params):

        """
        Train a model with a given set of hyperparameters and return the loss metric. 
        This function is intended to be used as a helper function within the `build_train_objective` function.

        Parameters:
            params: Dictionary of hyperparameters to use for training the model.

        Returns: 
            Dictionary containing the status of the run and the loss metric, where the loss metric is 
            1 minus the average test returns computed from the learning curve generated by fitting 
            the model with the given hyperparameters.

        """

        # Generate random code associated with an experimental run for distiguishability
        experiment_code = ''.join([random.choice(string.ascii_lowercase + string.digits) for _ in range(12)])
        
        # Gather metrics found from fitting a model on a given set of hyperparameters
        metrics = fit_and_log_learning_curve(train_data = train_data,
                                                target = target,
                                                features = features,
                                                horizon = horizon,
                                                window = window,
                                                params = params,
                                                scores = scores,
                                                model_type = model_type,
                                                model_class = model_class,
                                                run_name = f"{run_name}-{experiment_code}")
        
        # Return status of the run and the loss function
        return {'status': hyperopt.STATUS_OK,
                'loss': -metrics['avg_test_returns_lc']}
 
    return _train_func

def fit_and_log_learning_curve(train_data: pd.DataFrame,
                               target: str,
                               features: List[str],
                               horizon: int,
                               window: int,
                               params: Dict[str, Any],
                               scores: List[str],
                               model_type: str,
                               model_class: str,
                               run_name: str) -> Dict[str, Any]:
    
    """
    Fits a model on the given training data and extracts various learning metrics using a
    sliding window approach. These metrics are logged with MLflow to be visualized and
    compared.

    Parameters:
        train_data: A pandas DataFrame containing the training data.
        target: A string indicating the target variable.
        features: A list of strings indicating the feature names.
        horizon: An integer indicating the horizon to forecast.
        window: An integer indicating the window size for each step of the sliding window.
        params: A dictionary containing the hyperparameters to use for the model.
        scores: A list of strings indicating the metrics to optimize on.
        model_type: A string indicating the type of model to fit (either 'regressor' or 'classifier').
        model_class: A string indicating the model class to use.
        run_name: A string indicating the name of the MLflow run.

    Returns:
        A dictionary containing various learning metrics extracted from fitting a model on
        the given training data using a sliding window approach. These metrics are logged
        with MLflow and can be visualized and compared.
    """
    
    MODEL_ATTRIBUTES_COPY = deepcopy(MODEL_ATTRIBUTES)
    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
    with mlflow.start_run(nested = True, run_name = run_name) as run:
        # Fit CV models and extract predictions and metrics
        try:
            if target == 'direction':
                model_object = MODEL_ATTRIBUTES_COPY[target][model_class][model_type]
        except:
            raise ValueError
        
        try:
            if target == 'volatility_t_5':
                model_object = MODEL_ATTRIBUTES_COPY[target][model_type]
        except:
            raise ValueError

        model = model_object['model_object']
        model.set_params(**params)

        mlflow.log_params(params)

        #
        learning_assets = get_learning_scores(model = model,
                                              train_data = train_data,
                                              features = features,
                                              target = target,
                                              scores = scores,
                                              model_class = model_class,
                                              horizon = horizon,
                                              window = window,
                                              sklearn_metrics_fn = SKLEARN_METRICS_DICT)

        # Gather average and std of train + test metrics
        metrics_lc = {}
        metrics_lc['avg_train_returns_lc'] = np.mean(learning_assets['scores_dict']['train_returns'])
        metrics_lc['std_train_returns_lc'] = np.std(learning_assets['scores_dict']['train_returns'])
        metrics_lc['avg_test_returns_lc'] = np.mean(learning_assets['scores_dict']['test_returns'])
        metrics_lc['std_test_returns_lc'] = np.std(learning_assets['scores_dict']['test_returns'])

        mlflow.log_metrics(metrics_lc)

        for metric in scores:
            metrics_lc[metric] = {f"avg_train_{metric}_lc": np.mean(learning_assets['scores_dict'][f'train_{metric}']),
                                  f"std_train_{metric}_lc": np.std(learning_assets['scores_dict'][f'train_{metric}']),
                                  f"avg_test_{metric}_lc": np.mean(learning_assets['scores_dict'][f'test_{metric}']),
                                  f"std_test_{metric}_lc": np.std(learning_assets['scores_dict'][f'test_{metric}'])}
            

        # Log metrics
        for metric in scores:
            mlflow.log_metrics(metrics_lc[metric])

        return metrics_lc

def get_learning_scores(model: Any,
                        train_data: List[str],
                        features: List[str],
                        target: str,
                        scores: list, 
                        horizon: int,
                        window: int,
                        model_class: str,
                        sklearn_metrics_fn: Dict[str, str]):
    
    """
    Calculate scores for a learning model on a set of training data.
    
    Parameters:
        model: Learning model to train and predict on.
        train_data: List of training data.
        features: List of features used in the learning model.
        target: Name of the target column.
        scores: List of score metrics to calculate.
        horizon: Number of days to predict forward.
        window: Size of the rolling window to use when training the model.
        model_class: Class of the learning model ('ML' for machine learning or 'DL' for deep learning).
        sklearn_metrics_fn: Dictionary of sklearn metric functions.
    
    Returns:
        A dictionary containing the window sizes and scores calculated for each metric.
    """

    n = train_data.shape[0]
    X, y = train_data[features], train_data[target]
    window_sizes = list(range(n - window, n))
    scores_dict = defaultdict(list)
    for window_size in window_sizes:

        X_train, y_train = X.iloc[: window_size - horizon], y[: window_size - horizon]
        X_test, y_test = X.iloc[window_size - horizon: window_size], y[window_size - horizon : window_size]

        if model_class == 'ML':
            model.fit(X = X_train, y = y_train)

            train_preds = model.predict(X_train)
            test_preds = model.predict(X_test)
        
        else:
            train_preds = model.predict(None, X_train)
            test_preds = model.predict(None, X_test)

        for score in scores:
            scores_dict[f'train_{score}'].append(sklearn_metrics_fn[score](y_train, train_preds))
            scores_dict[f'test_{score}'].append(sklearn_metrics_fn[score](y_test, test_preds))

        train_preds[train_preds == 0] = -1
        test_preds[test_preds == 0] = -1

        train_returns = X_train['returns'].shift(-1)
        test_returns = X_test['returns'].shift(-1)

        scores_dict[f'train_returns'].append(sum((train_preds * train_returns).dropna()) / \
            sum(train_returns.dropna()))
        scores_dict[f'test_returns'].append(sum((test_preds * test_returns).dropna()) / \
            sum(test_returns.dropna()))

    return {'window_sizes': window_sizes,
            'scores_dict': scores_dict}

def fit_and_log_model(model_params: dict,
                      train_data: pd.DataFrame,
                      features: List[str],
                      target: str,
                      model_type: str,
                      model_class: str,
                      fitted_metric: str) -> None:
    
    """
    Fits the specified model on the training data, using the hyperparameters from the best run, and logs the trained model in MLflow.

    Parameters:
        model_params: A dictionary containing the best hyperparameters for the specified model.
        train_data: The training data to fit the model on.
        features: A list of features to use as input variables for the model.
        target: The target variable to predict.
        model_type: The type of model to fit.
        model_class: The class of model to fit - 'ML' for machine learning models and 'algo' for algorithmic models.
        fitted_metric: The name of the metric used to evaluate the fitted model.

    Returns:
        None
    """

    MODEL_ATTRIBUTES_COPY = deepcopy(MODEL_ATTRIBUTES)

    # Fit model based on parameters given
    if target == 'direction':
        try:
            model_assets = MODEL_ATTRIBUTES_COPY[target][model_class][model_type]
        except:
            raise ValueError

    elif target == 'volatility_t_5':
        try:
            model_assets = MODEL_ATTRIBUTES_COPY[target][model_type]
        except:
            raise ValueError

    # Load model object
    model = model_assets['model_object']
    model.set_params(**model_params["best_params"])

    # Get X and y data
    X, y = train_data[features], train_data[target]

    # Model fitting on best params
    if model_class == "ML":
        model.fit(X, y = y)
        eval(model_assets['mlflow_class']).log_model(model, 
                                                     registered_model_name = model_params["best_run_name"], 
                                                     artifact_path = "model")
    
    elif model_class == "algo":
        eval(model_assets['mlflow_class']).log_model(python_model = model, 
                                                     registered_model_name = model_params["best_run_name"], 
                                                     artifact_path = "model")


    mlflow.log_params(model_params["best_params"])
    # Log tags and parameters
    mlflow.set_tag("optimized_metric", fitted_metric)
    mlflow.set_tag("model_type", model_type)


def log_and_return_best_params(fitted_metric: str,
                               scores: list,
                               space: dict,
                               experiment_name: str) -> dict:
    
    """
    Logs the best set of hyperparameters and returns them.

    Parameters:
        fitted_metric: The metric that the model is optimized for.
        scores: List of scores that will be logged in MLflow.
        space: The hyperparameter space used to train the model.
        experiment_name: Name of the MLflow experiment used to search for the best run.

    Returns:
        A dictionary containing the best hyperparameters and the name of the best run.
    """

    # Search mlflow table and find runs with experiment name
    runs_info = mlflow.search_runs()
    runs = runs_info[runs_info["tags.mlflow.runName"].str.contains(experiment_name, na = False)]
    
    # Select rows associated with the best test metric
    best_run = runs.iloc[runs[f"metrics.avg_test_{fitted_metric}_lc"].argmax()]
    
    # Set tag using the name of the best experimental run
    mlflow.set_tag("best_run", best_run["tags.mlflow.runName"])
    
    # Store and log associated scores based on the best optimized metric
    best_metrics = {column.split(".")[1] : float(best_run[column]) for column in best_run.index if "metrics" in column and (any(score in column for score in scores) or "returns" in column)}
    mlflow.log_metrics(best_metrics)
    
    # Store and log optimal parameters
    best_params = {}
    for column in best_run.index:
        if "params" in column and any(param in column for param in space.keys()):
            try:
                # Converting the string containing tuple into just tuple.
                if type(eval(best_run[column])) == tuple:
                    best_params[column.split(".")[1]] = eval(best_run[column])
                elif int(float(best_run[column])) == float(best_run[column]):
                    best_params[column.split(".")[1]] = int(best_run[column])
                else:
                    best_params[column.split(".")[1]] = float(best_run[column])
            except:
                try:
                    if int(float(best_run[column])) == float(best_run[column]):
                        best_params[column.split(".")[1]] = int(best_run[column])
                    else:
                        best_params[column.split(".")[1]] = float(best_run[column])
                except:
                    best_params[column.split(".")[1]] = best_run[column]
    
    mlflow.log_params(best_params)
    
    return {"best_params": best_params, "best_run_name": best_run["tags.mlflow.runName"]}
